{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwUAmmTcl3G-"
      },
      "source": [
        "# Breaking Defenses & Black-Box Attacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wnLuXjkDmJWk",
        "outputId": "0281c8f5-e559-44de-b231-61d9012c671b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18, mobilenet_v2\n",
        "from torchvision.datasets.cifar import CIFAR10\n",
        "\n",
        "from tqdm import trange, tqdm\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YkpcqvmTcq6"
      },
      "source": [
        "# CIFAR10 Dataset (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iSIlVGXMoQA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137af703-596f-47fe-9559-f06920a7e3a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 28.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "norm_mean = (0.4914, 0.4822, 0.4465)\n",
        "norm_std = (0.2023, 0.1994, 0.2010)\n",
        "batch_size = 128\n",
        "\n",
        "mu = torch.tensor(norm_mean).view(3,1,1).to(device)\n",
        "std = torch.tensor(norm_std).view(3,1,1).to(device)\n",
        "\n",
        "# TODO: Set the upper limit and lower limit possible for images\n",
        "upper_limit = (1 - mu) / std\n",
        "lower_limit = (0 - mu) / std\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(norm_mean, norm_std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(norm_mean, norm_std),\n",
        "])\n",
        "\n",
        "trainset = CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_3oDmJlmTD-"
      },
      "source": [
        "# Defensive Distillation (25 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cJsiRusm54S"
      },
      "source": [
        "[Defensive distillation](https://arxiv.org/abs/1511.04508) proceeds in four steps:\n",
        "\n",
        "1.   **Train the teacher network**, by setting the temperature of the softmax to T during the\n",
        "training phase.\n",
        "2.   **Compute soft labels** by apply the teacher network to each instance in the training set, again evaluating the softmax at temperature T.\n",
        "3.  **Train the distilled network** (a network with the same shape as the teacher network) on the soft labels, using softmax at temperature T.\n",
        "4.  Finally, when running the distilled network at test time to classify new inputs, use temperature 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppg_6w0joa-D"
      },
      "source": [
        "## Train the teacher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rZj3ygtFocYN"
      },
      "outputs": [],
      "source": [
        "def train_step(model, dataloader, loss_fn, optimizer, temperature):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x) / temperature\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += x.size(0)\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    return total_loss / total, acc\n",
        "\n",
        "\n",
        "def train_teacher(model, n_epochs, loader=trainloader, temp=100):\n",
        "    model.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        loss, acc = train_step(model, loader, loss_fn, optimizer, temp)\n",
        "        print(f\"[Teacher] Epoch {epoch+1}/{n_epochs} | Loss: {loss:.4f} | Acc: {acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evuzvOUZ4bPT"
      },
      "source": [
        "You can use a pre-trained resnet to speed up the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_oKlYe7m4w-",
        "outputId": "d4f1f666-7919-4831-b428-fc9a6d1c7acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 71.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Teacher] Epoch 1/15 | Loss: 1.4461 | Acc: 62.78%\n",
            "[Teacher] Epoch 2/15 | Loss: 0.8037 | Acc: 74.28%\n",
            "[Teacher] Epoch 3/15 | Loss: 0.6556 | Acc: 78.26%\n",
            "[Teacher] Epoch 4/15 | Loss: 0.5899 | Acc: 79.99%\n",
            "[Teacher] Epoch 5/15 | Loss: 0.5396 | Acc: 81.75%\n",
            "[Teacher] Epoch 6/15 | Loss: 0.5101 | Acc: 82.67%\n",
            "[Teacher] Epoch 7/15 | Loss: 0.4751 | Acc: 83.79%\n",
            "[Teacher] Epoch 8/15 | Loss: 0.4463 | Acc: 84.79%\n",
            "[Teacher] Epoch 9/15 | Loss: 0.4335 | Acc: 85.26%\n",
            "[Teacher] Epoch 10/15 | Loss: 0.4090 | Acc: 85.77%\n",
            "[Teacher] Epoch 11/15 | Loss: 0.3944 | Acc: 86.52%\n",
            "[Teacher] Epoch 12/15 | Loss: 0.3804 | Acc: 86.95%\n",
            "[Teacher] Epoch 13/15 | Loss: 0.3663 | Acc: 87.45%\n",
            "[Teacher] Epoch 14/15 | Loss: 0.3540 | Acc: 87.73%\n",
            "[Teacher] Epoch 15/15 | Loss: 0.3411 | Acc: 88.14%\n"
          ]
        }
      ],
      "source": [
        "teacher = resnet18(pretrained=True)\n",
        "teacher.fc = nn.Linear(512, 10)\n",
        "train_teacher(teacher, 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EFhPyAmzmeB"
      },
      "source": [
        "## Test the teacher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iIVtvL32zopl"
      },
      "outputs": [],
      "source": [
        "def test_clean(model, dataloader=testloader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x).argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs9LVS7uN-Fp"
      },
      "source": [
        "Print the clean accuracy of the teacher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFEzA16Cz63A",
        "outputId": "99007cef-ce01-41e1-a2ee-c07082b07748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Accuracy 84.32%\n"
          ]
        }
      ],
      "source": [
        "print(f'Teacher Accuracy {test_clean(teacher):.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Fnkb39vYnl"
      },
      "source": [
        "## Train the student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nfu9ZVWFvaL5"
      },
      "outputs": [],
      "source": [
        "def distill(model, teacher, dataloader, optimizer, T):\n",
        "    model.train()\n",
        "    teacher.eval()\n",
        "\n",
        "    kl = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher(x) / T\n",
        "            soft_targets = F.softmax(teacher_logits, dim=1)\n",
        "\n",
        "        student_logits = model(x) / T\n",
        "        log_probs = F.log_softmax(student_logits, dim=1)\n",
        "\n",
        "        loss = kl(log_probs, soft_targets) * (T * T)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        preds = model(x).argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += x.size(0)\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    return total_loss / total, acc\n",
        "\n",
        "\n",
        "def train_student(model, teacher, n_epochs, loader=trainloader, temp=100):\n",
        "    model.to(device)\n",
        "    teacher.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        loss, acc = distill(model, teacher, loader, optimizer, temp)\n",
        "        print(f\"[Student] Epoch {epoch+1}/{n_epochs} | Loss: {loss:.4f} | Acc: {acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80y5gpfCIJjW"
      },
      "source": [
        "This time use a `resnet18` without the pretrained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhSefyQRwcFK",
        "outputId": "e0286572-d461-49d3-daab-a8a44c1f1bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Student] Epoch 1/15 | Loss: 13687.5224 | Acc: 40.46%\n",
            "[Student] Epoch 2/15 | Loss: 8849.4277 | Acc: 58.35%\n",
            "[Student] Epoch 3/15 | Loss: 6884.8912 | Acc: 65.99%\n",
            "[Student] Epoch 4/15 | Loss: 5613.3054 | Acc: 71.01%\n",
            "[Student] Epoch 5/15 | Loss: 4868.9447 | Acc: 73.62%\n",
            "[Student] Epoch 6/15 | Loss: 4268.5552 | Acc: 75.90%\n",
            "[Student] Epoch 7/15 | Loss: 3838.2570 | Acc: 77.63%\n",
            "[Student] Epoch 8/15 | Loss: 3498.4464 | Acc: 78.94%\n",
            "[Student] Epoch 9/15 | Loss: 3223.7420 | Acc: 79.68%\n",
            "[Student] Epoch 10/15 | Loss: 3049.4182 | Acc: 80.55%\n",
            "[Student] Epoch 11/15 | Loss: 2884.4784 | Acc: 81.21%\n",
            "[Student] Epoch 12/15 | Loss: 2656.5752 | Acc: 81.92%\n",
            "[Student] Epoch 13/15 | Loss: 2555.0217 | Acc: 82.31%\n",
            "[Student] Epoch 14/15 | Loss: 2477.2885 | Acc: 82.75%\n",
            "[Student] Epoch 15/15 | Loss: 2388.5061 | Acc: 83.09%\n"
          ]
        }
      ],
      "source": [
        "student = resnet18(weights=None)\n",
        "student.fc = nn.Linear(512, 10)\n",
        "train_student(student, teacher, 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snQvAMfH4ku1"
      },
      "source": [
        "## Test the student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_phh7vc4pA0",
        "outputId": "98c47375-cfab-4183-94a1-664410ebd3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Accuracy 81.01%\n"
          ]
        }
      ],
      "source": [
        "print(f'Student Accuracy {test_clean(student):.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdUmJKwtw-Gj"
      },
      "source": [
        "# Attack (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_-PnbtU3Zo1"
      },
      "source": [
        "Implement the FGSM attack and the `test_attack` funcion to report the robust accuracy for different values of epsilon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "axKc8-GExDh1"
      },
      "outputs": [],
      "source": [
        "def attack_fgsm(model, x, y, epsilon):\n",
        "    model.eval()\n",
        "\n",
        "    x_adv = x.clone().detach().requires_grad_(True)\n",
        "\n",
        "    logits = model(x_adv) / 100\n",
        "    loss = F.cross_entropy(logits, y)\n",
        "    loss.backward()\n",
        "\n",
        "    grad_sign = x_adv.grad.sign()\n",
        "    x_adv = x_adv + epsilon * grad_sign\n",
        "\n",
        "    x_adv = torch.max(torch.min(x_adv, upper_limit), lower_limit)\n",
        "\n",
        "    return x_adv.detach()\n",
        "\n",
        "\n",
        "def attack_pgd(model, x, y, epsilon, alpha=0.2, num_iters=10):\n",
        "    model.eval()\n",
        "\n",
        "    x_adv = x.clone().detach()\n",
        "    x_adv = x_adv + torch.empty_like(x_adv).uniform_(-epsilon, epsilon)\n",
        "    x_adv = torch.max(torch.min(x_adv, upper_limit), lower_limit)\n",
        "\n",
        "    for _ in range(num_iters):\n",
        "        x_adv.requires_grad_(True)\n",
        "\n",
        "        logits = model(x_adv) / 100\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        grad_sign = x_adv.grad.sign()\n",
        "        x_adv = x_adv + alpha * grad_sign\n",
        "\n",
        "        x_adv = torch.max(torch.min(x_adv, x + epsilon), x - epsilon)\n",
        "\n",
        "        x_adv = torch.max(torch.min(x_adv, upper_limit), lower_limit)\n",
        "\n",
        "        x_adv = x_adv.detach()\n",
        "\n",
        "    return x_adv\n",
        "\n",
        "\n",
        "def test_attack(model, epsilon, atttack=attack_fgsm, loader=testloader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        x_adv = atttack(model, x, y, epsilon)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = model(x_adv).argmax(dim=1)\n",
        "\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqUrHA1D1Oqk"
      },
      "source": [
        "Report the robust accuracy of the teacher for `ϵ = [1, 2, 4, 8, 16]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HMqzS21002f",
        "outputId": "b1c6d788-cae4-4a1d-9c8e-a216de19c80a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FGSM with ϵ=1/255 has Accuracy: 56.20%\n",
            "PGD  with ϵ=1/255 has Accuracy: 54.72%\n",
            "FGSM with ϵ=2/255 has Accuracy: 35.51%\n",
            "PGD  with ϵ=2/255 has Accuracy: 32.80%\n",
            "FGSM with ϵ=4/255 has Accuracy: 15.07%\n",
            "PGD  with ϵ=4/255 has Accuracy: 12.63%\n",
            "FGSM with ϵ=8/255 has Accuracy: 4.22%\n",
            "PGD  with ϵ=8/255 has Accuracy: 0.19%\n",
            "FGSM with ϵ=16/255 has Accuracy: 2.45%\n",
            "PGD  with ϵ=16/255 has Accuracy: 0.00%\n"
          ]
        }
      ],
      "source": [
        "epsilons = [1, 2, 4, 8, 16]\n",
        "scale = 1/std.mean().item()\n",
        "\n",
        "for eps in epsilons:\n",
        "    acc = test_attack(teacher, eps*scale/255, attack_fgsm)\n",
        "    print(f'FGSM with ϵ={eps}/255 has Accuracy: {acc:.2f}%')\n",
        "    acc = test_attack(teacher, eps*scale/255, attack_pgd)\n",
        "    print(f'PGD  with ϵ={eps}/255 has Accuracy: {acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd0E5Lrf3wyU"
      },
      "source": [
        "Do the same for the student:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn0oMzvk3wZR",
        "outputId": "b7b9de51-c556-467a-d81e-c2af38def983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FGSM with ϵ=1/255 has Accuracy: 73.09%\n",
            "PGD  with ϵ=1/255 has Accuracy: 72.58%\n",
            "FGSM with ϵ=2/255 has Accuracy: 72.53%\n",
            "PGD  with ϵ=2/255 has Accuracy: 72.19%\n",
            "FGSM with ϵ=4/255 has Accuracy: 72.38%\n",
            "PGD  with ϵ=4/255 has Accuracy: 71.89%\n",
            "FGSM with ϵ=8/255 has Accuracy: 72.33%\n",
            "PGD  with ϵ=8/255 has Accuracy: 71.57%\n",
            "FGSM with ϵ=16/255 has Accuracy: 72.31%\n",
            "PGD  with ϵ=16/255 has Accuracy: 69.12%\n"
          ]
        }
      ],
      "source": [
        "for eps in epsilons:\n",
        "    acc = test_attack(student, eps*scale/255, attack_fgsm)\n",
        "    print(f'FGSM with ϵ={eps}/255 has Accuracy: {acc:.2f}%')\n",
        "    acc = test_attack(student, eps*scale/255, attack_pgd)\n",
        "    print(f'PGD  with ϵ={eps}/255 has Accuracy: {acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgHqlsaa33j1"
      },
      "source": [
        "What do you see?\n",
        "\n",
        "The student’s accuracy does not drop much under attack and FGSM and PGD perform similarly which indicates gradient masking rather than true robustness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFbxKpeM4AR9"
      },
      "source": [
        "# Transferring Adversarial Examples (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Vu8xSsLeqN"
      },
      "source": [
        "Train yet another model to be used as the surrogate. (set temperature to 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qBouBbUKHoI",
        "outputId": "8ecda868-d255-428f-fe8b-130924e79674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Teacher] Epoch 1/10 | Loss: 1.5610 | Acc: 43.08%\n",
            "[Teacher] Epoch 2/10 | Loss: 1.1894 | Acc: 57.54%\n",
            "[Teacher] Epoch 3/10 | Loss: 1.0183 | Acc: 63.97%\n",
            "[Teacher] Epoch 4/10 | Loss: 0.9176 | Acc: 67.89%\n",
            "[Teacher] Epoch 5/10 | Loss: 0.8460 | Acc: 70.33%\n",
            "[Teacher] Epoch 6/10 | Loss: 0.7858 | Acc: 72.65%\n",
            "[Teacher] Epoch 7/10 | Loss: 0.7417 | Acc: 74.24%\n",
            "[Teacher] Epoch 8/10 | Loss: 0.6960 | Acc: 75.87%\n",
            "[Teacher] Epoch 9/10 | Loss: 0.6635 | Acc: 77.02%\n",
            "[Teacher] Epoch 10/10 | Loss: 0.6417 | Acc: 77.55%\n"
          ]
        }
      ],
      "source": [
        "surrogate = resnet18(weights=None)\n",
        "surrogate.fc = nn.Linear(512, 10)\n",
        "train_teacher(surrogate, 10, temp=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WBJtNiyL8WP"
      },
      "source": [
        "Print the surrogate accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdgr2I-VMCi3",
        "outputId": "879c1142-5ed8-4154-c7f8-bb197969c580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Surrogate Accuracy 77.05%\n"
          ]
        }
      ],
      "source": [
        "print(f'Surrogate Accuracy {test_clean(surrogate):.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcMBV3o6MC47"
      },
      "source": [
        "Report the accuracy of the surrogate for `ϵ = [1, 2, 4, 8, 16]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EALFUBHpMLdA",
        "outputId": "06f73d3e-3911-48ab-ee2f-12a56428fd1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FGSM with ϵ=1/255 has Accuracy: 61.33%\n",
            "FGSM with ϵ=2/255 has Accuracy: 45.28%\n",
            "FGSM with ϵ=4/255 has Accuracy: 22.58%\n",
            "FGSM with ϵ=8/255 has Accuracy: 5.78%\n",
            "FGSM with ϵ=16/255 has Accuracy: 1.13%\n"
          ]
        }
      ],
      "source": [
        "for eps in epsilons:\n",
        "    acc = test_attack(surrogate, eps*scale/255, attack_fgsm)\n",
        "    print(f'FGSM with ϵ={eps}/255 has Accuracy: {acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buQG6pvgLsAc"
      },
      "source": [
        "Implement the following functions to transfer attacks from a surrogate model to an oracle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Wu2vy0kY5Rmi"
      },
      "outputs": [],
      "source": [
        "def transfer_attack(oracle, model, eps, loader=testloader):\n",
        "    oracle.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x_adv = attack_fgsm(model, x, y, eps)\n",
        "        preds = oracle(x_adv).argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO24bT2p6oyl"
      },
      "source": [
        "Transfer attacks for `ϵ = [1, 2, 4, 8, 16]` from your model to the student."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qfcu7QgD6vtW",
        "outputId": "eb95b2f5-e941-48d8-f784-5fd4e7c6504a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FGSM with ϵ=1/255 has Accuracy: 75.77%\n",
            "FGSM with ϵ=2/255 has Accuracy: 69.70%\n",
            "FGSM with ϵ=4/255 has Accuracy: 56.54%\n",
            "FGSM with ϵ=8/255 has Accuracy: 33.67%\n",
            "FGSM with ϵ=16/255 has Accuracy: 13.42%\n"
          ]
        }
      ],
      "source": [
        "for eps in epsilons:\n",
        "    acc = transfer_attack(student, surrogate, eps*scale/255)\n",
        "    print(f'FGSM with ϵ={eps}/255 has Accuracy: {acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtGOcOmzPO3G"
      },
      "source": [
        "- What can be inferred from these results?\n",
        "\n",
        "DD does not provide real robustness and the model’s accuracy drops under adversarial attacks as epsilon increases\n",
        "\n",
        "- How are the accuracies of the student and the surrogate under attack related?\n",
        "\n",
        "They are closely related because adversarial examples transfer from the surrogate to the student and causing similar accuracy degradation\n",
        "\n",
        "- Does Defensive Distillation obfuscate the gradients? Why?\n",
        "\n",
        "Yes.\n",
        "high T training smooths the gradients but this only masks them and does not prevent successful iterative or transfer attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRPjp84p81nn"
      },
      "source": [
        "# ZOO Based Black-Box Attacks (25 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aaXmYSbQ2W_"
      },
      "source": [
        "Based on [Black-box Adversarial Attacks with Limited Queries and Information](https://arxiv.org/abs/1804.08598) you must first calculate the estimate of the graidents, and next attack the model based on your estimates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Z6PlkF_A-aDS"
      },
      "outputs": [],
      "source": [
        "def nes_gradient_estimate(model, x, y, epsilon, num_samples, sigma):\n",
        "    model.eval()\n",
        "    grad_est = torch.zeros_like(x)\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        u = torch.randn_like(x)\n",
        "        u = u / torch.norm(u, p=2)\n",
        "\n",
        "        x_pos = x + sigma * u\n",
        "        x_neg = x - sigma * u\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss_pos = F.cross_entropy(model(x_pos), y)\n",
        "            loss_neg = F.cross_entropy(model(x_neg), y)\n",
        "\n",
        "        grad_est += (loss_pos - loss_neg) * u\n",
        "\n",
        "    grad_est = grad_est / (2 * sigma * num_samples)\n",
        "    return grad_est"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2eq0ozYbmxp"
      },
      "source": [
        "I used 3 different things to estimate gradiant and all of them end up almost the same result. The bottom result is made with probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yBF7Spoh-nmJ"
      },
      "outputs": [],
      "source": [
        "def partial_information_attack(model, x, y, epsilon, num_samples, sigma, num_steps, alpha):\n",
        "    x_adv = x.clone().detach()\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        grad_est = nes_gradient_estimate(\n",
        "            model, x_adv, y, epsilon, num_samples, sigma\n",
        "        )\n",
        "\n",
        "        x_adv = x_adv + alpha * grad_est.sign()\n",
        "\n",
        "        x_adv = torch.max(torch.min(x_adv, x + epsilon), x - epsilon)\n",
        "\n",
        "        x_adv = torch.max(torch.min(x_adv, upper_limit), lower_limit)\n",
        "\n",
        "    return x_adv.detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8XgGVWZRb1P"
      },
      "source": [
        "Now run this attack on your models and report the results. (You **DON'T** need to run the attack for the entire test dataset as this will take a lot of time!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "N94Is59sRq8u"
      },
      "outputs": [],
      "source": [
        "def test_zoo_attack(model, epsilon, num_samples, sigma, num_steps, alpha, loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    total_batches = len(loader)\n",
        "    num_batches_to_use = max(1, int(0.5 * total_batches))\n",
        "\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        if i >= num_batches_to_use:\n",
        "            break\n",
        "\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        x_adv = partial_information_attack(\n",
        "            model, x, y, epsilon,\n",
        "            num_samples, sigma, num_steps, alpha\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = model(x_adv).argmax(dim=1)\n",
        "\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return 100 * correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "R_qAb4NADIxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a2a4c1-4a52-4340-e944-3fee2eee881e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZOO with ϵ=1/255 has Accuracy: 71.22%\n",
            "ZOO with ϵ=2/255 has Accuracy: 68.46%\n",
            "ZOO with ϵ=4/255 has Accuracy: 46.51%\n",
            "ZOO with ϵ=5/255 has Accuracy: 14.38%\n",
            "ZOO with ϵ=16/255 has Accuracy: 3.12%\n"
          ]
        }
      ],
      "source": [
        "epsilons = [1, 2, 4, 8, 16]\n",
        "\n",
        "for eps in epsilons:\n",
        "    acc = test_zoo_attack(model=surrogate, epsilon=eps*scale/255, num_samples=100, sigma=0.001, num_steps=10, alpha=0.1, loader=testloader)\n",
        "    print(f'ZOO with ϵ={eps}/255 has Accuracy: {acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P30qgaIHVFxr"
      },
      "source": [
        "# Adversarially Robust Distillation (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlP5yndiV7Fg"
      },
      "source": [
        "In this section we are going to test another type of distillation to see if this method is robust. This technique is [Adversarially Robust Distillation](https://arxiv.org/abs/1905.09747).\n",
        "\n",
        "\n",
        "\n",
        "1.   We will try to distill a robsut teacher from [Robust Bench](https://robustbench.github.io/) onto a smaller architecture.\n",
        "2.   We minimize the KL-Divergence between the logits of the student and teacher to ensure fidelity. (You can also incorporate the classification loss as mentioned in the paper but you can choose to ignore it as well)\n",
        "3.   At each step of the distillation you will attack the student (you can use either FGSM or PGD) and find an adversarial example $X + \\delta$ for data point $X$. Next you will minimize $t^2 \\times \\text{KL}(S(X+\\delta), T(X))$ where $S$ and $T$ are the student and teacher networks respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "kOHVJtf0V6NT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21562a9-3a05-47a9-a6f8-a6522ccc8308"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/RobustBench/robustbench.git\n",
            "  Cloning https://github.com/RobustBench/robustbench.git to /tmp/pip-req-build-r8ifwdco\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/RobustBench/robustbench.git /tmp/pip-req-build-r8ifwdco\n",
            "  Resolved https://github.com/RobustBench/robustbench.git to commit 78fcc9e48a07a861268f295a777b975f25155964\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting autoattack@ git+https://github.com/fra31/auto-attack.git@a39220048b3c9f2cca9a4d3a54604793c68eca7e#egg=autoattack (from robustbench==1.1)\n",
            "  Cloning https://github.com/fra31/auto-attack.git (to revision a39220048b3c9f2cca9a4d3a54604793c68eca7e) to /tmp/pip-install-ny6hf4fi/autoattack_6b2b9fdd07054b7a8195b0ae167c05cb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/fra31/auto-attack.git /tmp/pip-install-ny6hf4fi/autoattack_6b2b9fdd07054b7a8195b0ae167c05cb\n",
            "  Running command git rev-parse -q --verify 'sha^a39220048b3c9f2cca9a4d3a54604793c68eca7e'\n",
            "  Running command git fetch -q https://github.com/fra31/auto-attack.git a39220048b3c9f2cca9a4d3a54604793c68eca7e\n",
            "  Resolved https://github.com/fra31/auto-attack.git to commit a39220048b3c9f2cca9a4d3a54604793c68eca7e\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from robustbench==1.1) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.12/dist-packages (from robustbench==1.1) (0.24.0+cu126)\n",
            "Collecting torchdiffeq (from robustbench==1.1)\n",
            "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Collecting geotorch (from robustbench==1.1)\n",
            "  Downloading geotorch-0.3.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.12/dist-packages (from robustbench==1.1) (2.32.4)\n",
            "Requirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.12/dist-packages (from robustbench==1.1) (2.0.2)\n",
            "Requirement already satisfied: Jinja2~=3.1.2 in /usr/local/lib/python3.12/dist-packages (from robustbench==1.1) (3.1.6)\n",
            "Requirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from robustbench==1.1) (4.67.1)\n",
            "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.12/dist-packages (from robustbench==1.1) (2.2.2)\n",
            "Requirement already satisfied: timm>=1.0.9 in /usr/local/lib/python3.12/dist-packages (from robustbench==1.1) (1.0.22)\n",
            "Collecting gdown==5.1.0 (from robustbench==1.1)\n",
            "  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from robustbench==1.1) (6.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown==5.1.0->robustbench==1.1) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown==5.1.0->robustbench==1.1) (3.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2~=3.1.2->robustbench==1.1) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.5->robustbench==1.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.5->robustbench==1.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.5->robustbench==1.1) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->robustbench==1.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->robustbench==1.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->robustbench==1.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.25.0->robustbench==1.1) (2025.11.12)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.9->robustbench==1.1) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.9->robustbench==1.1) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (3.6.1)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.1->robustbench==1.1) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.8.2->robustbench==1.1) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from torchdiffeq->robustbench==1.1) (1.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->robustbench==1.1) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7.1->robustbench==1.1) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown==5.1.0->robustbench==1.1) (2.8)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm>=1.0.9->robustbench==1.1) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm>=1.0.9->robustbench==1.1) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown==5.1.0->robustbench==1.1) (1.7.1)\n",
            "Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
            "Downloading geotorch-0.3.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Building wheels for collected packages: robustbench, autoattack\n",
            "  Building wheel for robustbench (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for robustbench: filename=robustbench-1.1-py3-none-any.whl size=194417 sha256=b321faf248e844faeab2e39e6a8c787bd75db0a2e59fdcc71e43ccf371597636\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uc_lzrvc/wheels/c8/87/e4/8a18b14c4fe0a3164c2c924d4f04d50856a106871651cb6513\n",
            "  Building wheel for autoattack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autoattack: filename=autoattack-0.1-py3-none-any.whl size=36228 sha256=cae0e0c0bde88379a9e9fdefcf5e2a0c18864bce712f7d84407d24810d5722c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/ae/a7/15224c2864e3bdd6d8eac1f7210ced465a650061a5fb11b502\n",
            "Successfully built robustbench autoattack\n",
            "Installing collected packages: autoattack, gdown, torchdiffeq, geotorch, robustbench\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 5.2.0\n",
            "    Uninstalling gdown-5.2.0:\n",
            "      Successfully uninstalled gdown-5.2.0\n",
            "Successfully installed autoattack-0.1 gdown-5.1.0 geotorch-0.3.0 robustbench-1.1 torchdiffeq-0.2.5\n"
          ]
        }
      ],
      "source": [
        "! pip install git+https://github.com/RobustBench/robustbench.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "nD9wjBojWz_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c39a64-d191-4948-d4a2-820232d4834d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading models/cifar10/Linf/Gowal2021Improving_R18_ddpm_100m.pt (gdrive_id=1-0EuCJashqSXEkkd1DOzFA4tH8KL2kim).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1-0EuCJashqSXEkkd1DOzFA4tH8KL2kim\n",
            "From (redirected): https://drive.google.com/uc?id=1-0EuCJashqSXEkkd1DOzFA4tH8KL2kim&confirm=t&uuid=1c031c21-0c63-4b4f-8edd-e533e197bbba\n",
            "To: /content/models/cifar10/Linf/Gowal2021Improving_R18_ddpm_100m.pt\n",
            "100%|██████████| 50.3M/50.3M [00:00<00:00, 59.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "from robustbench.utils import load_model\n",
        "\n",
        "teacher = load_model(model_name='Gowal2021Improving_R18_ddpm_100m', dataset='cifar10', threat_model='Linf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "FLGDeE1YY6uO"
      },
      "outputs": [],
      "source": [
        "def ard(student, teacher, dataloader, optimizer, eps, attack):\n",
        "    student.train()\n",
        "    teacher.eval()\n",
        "\n",
        "    kl = nn.KLDivLoss(reduction=\"batchmean\")\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for x, y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        x_adv = attack(student, x, y, eps)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher(x)\n",
        "            teacher_probs = F.softmax(teacher_logits, dim=1)\n",
        "\n",
        "        student_logits = student(x_adv)\n",
        "        student_log_probs = F.log_softmax(student_logits, dim=1)\n",
        "\n",
        "        loss = kl(student_log_probs, teacher_probs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        preds = student(x).argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += x.size(0)\n",
        "\n",
        "    return total_loss / total, 100 * correct / total\n",
        "\n",
        "\n",
        "\n",
        "def adv_train_student(model, teacher, n_epochs, eps=8/255, loader=trainloader):\n",
        "    model.to(device)\n",
        "    teacher.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        loss, acc = ard(model, teacher, loader, optimizer, eps, attack_fgsm)\n",
        "        print(f\"[ARD] Epoch {epoch+1}/{n_epochs} | Loss: {loss:.4f} | Acc: {acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMsIxPr5Opku",
        "outputId": "091a9981-2860-44c1-dd3c-c2461f6eea62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15] - Loss: 2.8451, Accuracy: 16.92%\n",
            "Epoch [2/15] - Loss: 2.3718, Accuracy: 21.08%\n",
            "Epoch [3/15] - Loss: 2.2146, Accuracy: 22.87%\n",
            "Epoch [4/15] - Loss: 2.1439, Accuracy: 24.91%\n",
            "Epoch [5/15] - Loss: 2.0872, Accuracy: 26.43%\n",
            "Epoch [6/15] - Loss: 2.0285, Accuracy: 26.01%\n",
            "Epoch [7/15] - Loss: 1.9724, Accuracy: 28.95%\n",
            "Epoch [8/15] - Loss: 1.9461, Accuracy: 29.67%\n",
            "Epoch [9/15] - Loss: 1.9218, Accuracy: 31.42%\n",
            "Epoch [10/15] - Loss: 1.8986, Accuracy: 30.88%\n",
            "Epoch [11/15] - Loss: 1.8627, Accuracy: 32.76%\n",
            "Epoch [12/15] - Loss: 1.8349, Accuracy: 33.54%\n",
            "Epoch [13/15] - Loss: 1.8183, Accuracy: 32.97%\n",
            "Epoch [14/15] - Loss: 1.7694, Accuracy: 34.61%\n",
            "Epoch [15/15] - Loss: 1.7532, Accuracy: 33.85%\n"
          ]
        }
      ],
      "source": [
        "student = mobilenet_v2(weights=None)\n",
        "student.classifier[1] = nn.Linear(student.last_channel, 10)\n",
        "\n",
        "adv_train_student(student, teacher, n_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwVcIMu9ZF12"
      },
      "source": [
        "Now report the accuracy of the student on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oxo-42hUtJ0",
        "outputId": "14d4b6cd-e237-4b60-c204-8a704f71e22a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Student Accuracy: 27.63%\n",
            "FGSM with ϵ=8/255 has Accuracy: 16.16%\n",
            "PGD with ϵ=8/255 has Accuracy: 14.98%\n"
          ]
        }
      ],
      "source": [
        "print(\"Student Accuracy:\", test_clean(student))\n",
        "\n",
        "print(\"FGSM with ϵ=8/255 has Accuracy:\", test_attack(student, 8/255, attack_fgsm))\n",
        "\n",
        "print(\"PGD with ϵ=8/255 has Accuracy:\", test_attack(student, 8/255, attack_pgd))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}